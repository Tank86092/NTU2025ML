{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tank86092/2025ML/blob/main/2025ML_HW1_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **2025 ML FALL HW1: PM2.5 Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeZnPAiwDRWG"
      },
      "source": [
        "Author: MLTAs\n",
        "\n",
        "Methods:\n",
        "* Training with all data\n",
        "* Optimizer: RMSProp (default)\n",
        "* TODOs:\n",
        "  - Complete the `valid()` function\n",
        "  - Tune the hyperparameters in `train_config`\n",
        "  - Implement 2nd-order polynomial regression model (without interaction terms) in `minibatch_2()`\n",
        "  - Implement feature normalization in `normalize_train_data()`\n",
        "  - Feature selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_4-77xHk44"
      },
      "source": [
        "# **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1919,
      "metadata": {
        "id": "k-onQd4JNA5H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqMEWsRekx0L"
      },
      "source": [
        "# **Fix random seed**\n",
        "\n",
        "\n",
        "This is for the reproduction of your result. **DO NOT modify this secton!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1920,
      "metadata": {
        "id": "UxDA6fJb_Uem"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Seed: 2600395669\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "seed = time.time().as_integer_ratio()[0] % (2**32 - 1)\n",
        "print(\"Random Seed:\", seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OVRMuTAc1_E"
      },
      "source": [
        "# **Download training data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1921,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0Zo8JUp5kJ4",
        "outputId": "b9204435-62e0-4946-a0cf-a8d094051c25"
      },
      "outputs": [],
      "source": [
        "# !gdown --id \"1Hfzrcm69QwdFvdeF0uASoQlcVxKw_hHy\" --output \"train.csv\"\n",
        "# !gdown --id '155N6fzI7vAFzHAGdy6jkaWIksWH6Y1G2' --output \"test.csv\"\n",
        "\n",
        "# Incase the links above die, you can use the following instead.\n",
        "#!gdown --id '11abE854Eyv4BA7qt5k8r_80sJ3KuOQUN' --output \"train.csv\"\n",
        "#!gdown --id '1uod-Z4ztluXnuHtgUbm39nMudUKqXHMl' --output \"test.csv\"\n",
        "\n",
        "# If the data is still missing, you can manually download it from kaggle, and upload the files under /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1922,
      "metadata": {
        "id": "yHpuZmQwXpz8"
      },
      "outputs": [],
      "source": [
        "def valid(x, y,norm_params):\n",
        "  # TODO: Try to filter out extreme values.\n",
        "  #  ex: If PM2.5 > 100, then we don't use the data to train (return False), otherwise return True,\n",
        "  # if norm_params is not None:\n",
        "  #   var_sum = 0.0\n",
        "  #   for i in range(x.shape[0]):\n",
        "  #     for j in range(x.shape[1]-1):\n",
        "  #       var_sum += x[i,j] ** 2\n",
        "  #   var_mean = var_sum / (x.shape[0] * (x.shape[1]-1))\n",
        "  #   if var_mean > 16.0:\n",
        "  #     return False\n",
        "  \n",
        "  # for i in range(x.shape[0]):\n",
        "  #     if x[i,-1] > 20:\n",
        "  #       return False\n",
        "  if y > 20:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "\n",
        "# Create your dataset\n",
        "def parse2train(data, feats,norm_params=None):\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  # Use data #0~#7 to predict #8 => Total data length should be decresased by 8.\n",
        "  total_length = data.shape[1] - 8\n",
        "\n",
        "  for i in range(total_length):\n",
        "    x_tmp = data[feats,i:i+8 ] # Use data #0~#7 to predict #8, data #1~#8 to predict #9, etc.\n",
        "    y_tmp = data[-1, i+8] # last column of (i+8)th row: PM2.5\n",
        "    # Filter out extreme values to train.\n",
        "    if valid(x_tmp, y_tmp,norm_params):\n",
        "      x.append(x_tmp.reshape(-1,))\n",
        "      y.append(y_tmp)\n",
        "\n",
        "  # x.shape: (n, 15, 8)\n",
        "  # y.shape: (n, 1)\n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "\n",
        "  return x,y\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyEpvVVQdZ0c"
      },
      "source": [
        "#**Gradient descent**\n",
        "###**RMSProp**\n",
        "1. $v_t=\\beta \\cdot v_{t-1} + (1-\\beta)(\\nabla w_t)^2$\n",
        "2. $w_{t+1}=w_t - \\frac{\\eta}{\\sqrt{(v_t)}+\\epsilon}\\nabla w_t$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* This is our gradient descent algorithm. RMSProp was implemented in `minibatch()`.\n",
        "* You can implement other algorithm, such as SGD or other gradient descent variants listed below, which may (or may not) improve performance.\n",
        "* However, **modules like sklearn and pytorch are not allowed!!!**\n",
        "* Ref:\n",
        "  - Prof. G. Hinton's lecture: https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
        "  - Prof. Hung-Yi Lee's video: https://youtu.be/HYUXEeh3kwY?si=RtLjSj51WK1pmz87\n",
        "\n",
        "###**Adam (RMSProp + Momemtum)**\n",
        "* Ref:\n",
        "  - Paper: https://arxiv.org/pdf/1412.6980\n",
        "  - Prof. Hung-Yi Lee's video: https://youtu.be/HYUXEeh3kwY?si=RtLjSj51WK1pmz87\n",
        "\n",
        "###**AdamW (Adam with decoupled weight decay)**\n",
        "* Ref:\n",
        "  - Paper: https://arxiv.org/pdf/1711.05101\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcofxZ8c4kZE"
      },
      "outputs": [],
      "source": [
        "def minibatch(x, y, config):\n",
        "    # Randomize the data in minibatch\n",
        "    index = np.arange(x.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    y = y[index]\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "    # Initialization\n",
        "    batch_size = config.batch_size\n",
        "    lr = config.lr\n",
        "    epoch = config.epoch\n",
        "    decay_rate = config.decay_rate\n",
        "    reg = config.reg\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Linear regression: only contains two parameters (w, b).\n",
        "    w = np.random.normal(0,1,x[0].shape).reshape(-1, 1)\n",
        "    bias = 0.1\n",
        "\n",
        "    # Optimizer states\n",
        "    cache_w = np.zeros_like(w)\n",
        "    cache_b = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for num in range(epoch):\n",
        "        grad = 0\n",
        "        loss_sum = 0\n",
        "        for b in range(int(x.shape[0] / batch_size)):\n",
        "            x_batch = x[b * batch_size:(b + 1) * batch_size]\n",
        "            y_batch = y[b * batch_size:(b + 1) * batch_size].reshape(-1, 1)\n",
        "\n",
        "            # Prediction of linear regression\n",
        "            pred = np.dot(x_batch, w) + bias\n",
        "\n",
        "            # Loss\n",
        "            loss = y_batch - pred\n",
        "            loss_sum += (loss**2).sum() + reg * (w**2).sum()\n",
        "\n",
        "            # Compute gradient\n",
        "            g_t = np.dot(x_batch.transpose(), loss) * (-2) + 2 * reg * w\n",
        "            g_t_b = loss.sum(axis=0) * (-2)\n",
        "            grad += np.mean(g_t**2)\n",
        "\n",
        "            # Update cache\n",
        "            cache_w = decay_rate * cache_w + (1 - decay_rate) * g_t**2\n",
        "            cache_b = decay_rate * cache_b + (1 - decay_rate) * g_t_b**2\n",
        "\n",
        "            # Update weight & bias\n",
        "            w -= lr * g_t / (np.sqrt(cache_w) + epsilon)\n",
        "            bias -= lr * g_t_b / (np.sqrt(cache_b) + epsilon)\n",
        "        print(\"Epoch %d/%d\" % (num+1, epoch),end=' ')\n",
        "        print(\"loss = \", loss_sum/x.shape[0],end=' ')\n",
        "        print(\"gradiant norm = \", grad/x.shape[0])\n",
        "\n",
        "    return w, bias\n",
        "\n",
        "# TODO: Implement 2-nd polynomial regression version for the report.\n",
        "def minibatch_2(x, y, config):# Randomize the data in minibatch\n",
        "    index = np.arange(x.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    x2 = x ** 2\n",
        "    x = np.concatenate((x, x2), axis=1)\n",
        "    y = y[index]\n",
        "\n",
        "    # Initialization\n",
        "    batch_size = config.batch_size\n",
        "    lr = config.lr\n",
        "    epoch = config.epoch\n",
        "    decay_rate = config.decay_rate\n",
        "    reg = config.reg\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Linear regression: only contains two parameters (w, b).\n",
        "    w = np.full(x.shape[1], 0.1).reshape(-1, 1)\n",
        "    bias = 0.1\n",
        "\n",
        "    # Optimizer states\n",
        "    cache_w = np.zeros_like(w)\n",
        "    cache_b = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for num in range(epoch):\n",
        "        loss_sum = 0\n",
        "        grad = 0\n",
        "        if num % 10 == 0:\n",
        "            lr /= 10\n",
        "        for b in range(int(x.shape[0] / batch_size)):\n",
        "            x_batch = x[b * batch_size:(b + 1) * batch_size]\n",
        "            y_batch = y[b * batch_size:(b + 1) * batch_size].reshape(-1, 1)\n",
        "\n",
        "            # Prediction of linear regression\n",
        "            pred = np.dot(x_batch, w) + bias\n",
        "\n",
        "            # Loss\n",
        "            loss = y_batch - pred\n",
        "            loss_sum += (loss**2).sum() + reg * (w**2).sum()\n",
        "\n",
        "            # Compute gradient\n",
        "            # implemtation of adam optimizer\n",
        "            g_t = np.dot(x_batch.transpose(), loss) * (-2) + 2 * reg * w\n",
        "            \n",
        "            g_t_b = loss.sum(axis=0) * (-2)\n",
        "\n",
        "            grad += np.mean(g_t**2)\n",
        "            # Update cache\n",
        "            cache_w = decay_rate * cache_w + (1 - decay_rate) * g_t**2\n",
        "            cache_b = decay_rate * cache_b + (1 - decay_rate) * g_t_b**2\n",
        "\n",
        "            # Update weight & bias\n",
        "            w -= lr * g_t / (np.sqrt(cache_w) + epsilon)\n",
        "            bias -= lr * g_t_b / (np.sqrt(cache_b) + epsilon)\n",
        "        print(\"Epoch %d/%d\" % (num+1, epoch),end=' ')\n",
        "        print(\"loss = \", loss_sum/x.shape[0],end=' ')\n",
        "        print(\"gradiant norm = \", grad/x.shape[0])\n",
        "\n",
        "    return w, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1924,
      "metadata": {
        "id": "ZpdOsMfXLxH2"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "# TODO: Tune the config to boost your performance.\n",
        "train_config = Namespace(\n",
        "    batch_size = 8,\n",
        "    lr = 0.01,\n",
        "    epoch = 50,\n",
        "    decay_rate = 0.9,\n",
        "    reg = 0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay-RhqqA88vS"
      },
      "source": [
        "# **Training your regression model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1925,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "EoR5Q5kvJm4t",
        "outputId": "5e24e2ff-04ef-4f18-acd7-755d5ba6371f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AMB_TEMP</th>\n",
              "      <th>CO</th>\n",
              "      <th>NO</th>\n",
              "      <th>NO2</th>\n",
              "      <th>NOx</th>\n",
              "      <th>O3</th>\n",
              "      <th>PM10</th>\n",
              "      <th>WS_HR</th>\n",
              "      <th>RAINFALL</th>\n",
              "      <th>RH</th>\n",
              "      <th>SO2</th>\n",
              "      <th>WD_HR</th>\n",
              "      <th>WIND_DIREC</th>\n",
              "      <th>WIND_SPEED</th>\n",
              "      <th>PM2.5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.8</td>\n",
              "      <td>0.32</td>\n",
              "      <td>1.7</td>\n",
              "      <td>8.6</td>\n",
              "      <td>10.3</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71</td>\n",
              "      <td>1.9</td>\n",
              "      <td>172</td>\n",
              "      <td>171</td>\n",
              "      <td>0.6</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.8</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.6</td>\n",
              "      <td>6.2</td>\n",
              "      <td>7.8</td>\n",
              "      <td>23.8</td>\n",
              "      <td>20</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71</td>\n",
              "      <td>1.7</td>\n",
              "      <td>161</td>\n",
              "      <td>129</td>\n",
              "      <td>1.8</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.9</td>\n",
              "      <td>5.4</td>\n",
              "      <td>6.3</td>\n",
              "      <td>27.4</td>\n",
              "      <td>21</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68</td>\n",
              "      <td>1.6</td>\n",
              "      <td>152</td>\n",
              "      <td>147</td>\n",
              "      <td>1.5</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.0</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>29.5</td>\n",
              "      <td>21</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68</td>\n",
              "      <td>1.6</td>\n",
              "      <td>138</td>\n",
              "      <td>145</td>\n",
              "      <td>1.7</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11.3</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.8</td>\n",
              "      <td>30.7</td>\n",
              "      <td>16</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67</td>\n",
              "      <td>1.6</td>\n",
              "      <td>140</td>\n",
              "      <td>139</td>\n",
              "      <td>1.7</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5769</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.41</td>\n",
              "      <td>1.2</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>23.0</td>\n",
              "      <td>21</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74</td>\n",
              "      <td>2.8</td>\n",
              "      <td>149</td>\n",
              "      <td>168</td>\n",
              "      <td>2.0</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5770</th>\n",
              "      <td>28.2</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.7</td>\n",
              "      <td>11.7</td>\n",
              "      <td>13.5</td>\n",
              "      <td>19.5</td>\n",
              "      <td>23</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>78</td>\n",
              "      <td>2.3</td>\n",
              "      <td>187</td>\n",
              "      <td>179</td>\n",
              "      <td>2.5</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5771</th>\n",
              "      <td>28.0</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.3</td>\n",
              "      <td>9.1</td>\n",
              "      <td>10.4</td>\n",
              "      <td>17.6</td>\n",
              "      <td>17</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>78</td>\n",
              "      <td>2.0</td>\n",
              "      <td>173</td>\n",
              "      <td>200</td>\n",
              "      <td>1.5</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5772</th>\n",
              "      <td>28.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.4</td>\n",
              "      <td>9.5</td>\n",
              "      <td>11.0</td>\n",
              "      <td>15.4</td>\n",
              "      <td>17</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75</td>\n",
              "      <td>1.8</td>\n",
              "      <td>171</td>\n",
              "      <td>135</td>\n",
              "      <td>0.9</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5773</th>\n",
              "      <td>27.9</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.6</td>\n",
              "      <td>8.3</td>\n",
              "      <td>9.0</td>\n",
              "      <td>14.6</td>\n",
              "      <td>15</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>76</td>\n",
              "      <td>1.4</td>\n",
              "      <td>138</td>\n",
              "      <td>136</td>\n",
              "      <td>1.1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5774 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      AMB_TEMP    CO   NO   NO2   NOx    O3  PM10  WS_HR  RAINFALL  RH  SO2  \\\n",
              "0         10.8  0.32  1.7   8.6  10.3  22.9    21    0.6       0.0  71  1.9   \n",
              "1         10.8  0.27  1.6   6.2   7.8  23.8    20    1.4       0.0  71  1.7   \n",
              "2         11.0  0.25  0.9   5.4   6.3  27.4    21    0.8       0.0  68  1.6   \n",
              "3         11.0  0.23  0.7   3.1   3.8  29.5    21    1.8       0.0  68  1.6   \n",
              "4         11.3  0.22  0.8   2.9   3.8  30.7    16    1.9       0.0  67  1.6   \n",
              "...        ...   ...  ...   ...   ...   ...   ...    ...       ...  ..  ...   \n",
              "5769      29.0  0.41  1.2  14.0  15.3  23.0    21    1.4       0.0  74  2.8   \n",
              "5770      28.2  0.33  1.7  11.7  13.5  19.5    23    2.1       0.0  78  2.3   \n",
              "5771      28.0  0.29  1.3   9.1  10.4  17.6    17    1.5       0.0  78  2.0   \n",
              "5772      28.0  0.27  1.4   9.5  11.0  15.4    17    1.1       0.0  75  1.8   \n",
              "5773      27.9  0.23  0.6   8.3   9.0  14.6    15    0.8       0.0  76  1.4   \n",
              "\n",
              "      WD_HR  WIND_DIREC  WIND_SPEED  PM2.5  \n",
              "0       172         171         0.6     15  \n",
              "1       161         129         1.8     13  \n",
              "2       152         147         1.5     12  \n",
              "3       138         145         1.7      9  \n",
              "4       140         139         1.7      9  \n",
              "...     ...         ...         ...    ...  \n",
              "5769    149         168         2.0     14  \n",
              "5770    187         179         2.5     15  \n",
              "5771    173         200         1.5     13  \n",
              "5772    171         135         0.9     10  \n",
              "5773    138         136         1.1      9  \n",
              "\n",
              "[5774 rows x 15 columns]"
            ]
          },
          "execution_count": 1925,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"./content/train.csv\")\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1926,
      "metadata": {
        "id": "5RbM6K-e6dTz"
      },
      "outputs": [],
      "source": [
        "# TODO: Normalize each column (except PM2.5) for the report (use z-score normalization)\n",
        "def normalize_train_data(df):\n",
        "\n",
        "    \"\"\"\n",
        "    Steps:\n",
        "    1. For each column (except PM2.5): calculate mean and std\n",
        "    2. Apply standardization: (column - mean) / std\n",
        "    3. Store normalization parameters for later use on test data\n",
        "\n",
        "    Returns:\n",
        "        normalized_df: DataFrame with normalized features\n",
        "        norm_params: Dict with {'column': {'mean': X, 'std': Y}}\n",
        "\n",
        "    Hint: Loop through data.columns, skip PM2.5\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    mean = df.mean(axis=0)\n",
        "    std = df.std(axis=0)\n",
        "\n",
        "    data_norm = df.copy()\n",
        "    for col in df.columns:\n",
        "        if col != 'PM2.5':\n",
        "            data_norm[col] = (df[col] - mean[col]) / std[col]\n",
        "            \n",
        "    norm_params = {}\n",
        "    mean = mean.to_dict()\n",
        "    std = std.to_dict()\n",
        "    for key in mean.keys():\n",
        "        if key != 'PM2.5':\n",
        "            norm_params[key] = {'mean': mean[key], 'std': std[key]}\n",
        "    return data_norm, norm_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1927,
      "metadata": {
        "id": "_Akqj5yYVGHA"
      },
      "outputs": [],
      "source": [
        "# Choose your features to train.\n",
        "# Hint:\n",
        "# 1. You can select more than one feature.\n",
        "# 2. You should select \"good\" features.\n",
        "\n",
        "# TODO: Carefully justify which feature should be chosen.\n",
        "# 0:AMB_TEMP,1:CO, 2:NO,3:NO2,4:NOx,5:O3,6:PM10,7:WS_HR,8:RAINFALL,9:RH,10:SO2,11:WD_HR,12:WIND_DIREC,13:WIND_SPEED,14:PM2.5\n",
        "\n",
        "feats = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1928,
      "metadata": {
        "id": "AiEWGMQXLM99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "optimal training Loss : 6.960187139485019\n"
          ]
        }
      ],
      "source": [
        "# Training data preprocessing\n",
        "def train_processing(train_df, norm=False):\n",
        "    \"\"\"Process training train_df with optional normalization\"\"\"\n",
        "\n",
        "    if norm:\n",
        "        # Normalize training data and save parameters (mean & std)\n",
        "        data_norm, norm_params = normalize_train_data(train_df)\n",
        "        data_values = data_norm.values\n",
        "    else:\n",
        "        # Use raw training data\n",
        "        data_values = train_df.values\n",
        "        norm_params = None\n",
        "\n",
        "    # Common processing steps\n",
        "    train_data = np.transpose(np.array(np.float64(data_values)))\n",
        "    train_x, train_y = parse2train(train_data, feats,norm_params)\n",
        "\n",
        "    return train_x, train_y, norm_params\n",
        "\n",
        "train_x, train_y, norm_params = train_processing(train_df, norm=True)\n",
        "X = np.hstack((train_x, np.full(train_x.shape[0],1).reshape(-1,1)))\n",
        "X_pinv = np.linalg.pinv(X)\n",
        "theta = X_pinv @ train_y\n",
        "loss = np.sum((train_y - X @ theta) ** 2) / train_y.shape[0]\n",
        "print(f\"optimal training Loss : {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1929,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhfoPJUhcnH9",
        "outputId": "d53d4c90-bdbe-4f87-b1a7-f1e27a6398f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 loss =  2474.6884340694955 gradiant norm =  323813452.7454778\n",
            "Epoch 2/50 loss =  11.537346272564148 gradiant norm =  568550.6654525978\n",
            "Epoch 3/50 loss =  10.342796593015159 gradiant norm =  423636.1890053651\n",
            "Epoch 4/50 loss =  10.05871076309556 gradiant norm =  563139.7954021156\n",
            "Epoch 5/50 loss =  9.841322563709396 gradiant norm =  633958.2799793192\n",
            "Epoch 6/50 loss =  9.596938640243977 gradiant norm =  653237.2603600052\n",
            "Epoch 7/50 loss =  9.539018318204592 gradiant norm =  669072.0044683467\n",
            "Epoch 8/50 loss =  9.466446431453562 gradiant norm =  673103.1934876782\n",
            "Epoch 9/50 loss =  9.425339658424916 gradiant norm =  673392.389714973\n",
            "Epoch 10/50 loss =  9.365312640577665 gradiant norm =  674976.9438536685\n",
            "Epoch 11/50 loss =  7.4645776633665 gradiant norm =  100634.09586672895\n",
            "Epoch 12/50 loss =  7.2979505876440625 gradiant norm =  79097.58894855651\n",
            "Epoch 13/50 loss =  7.267545369224984 gradiant norm =  77593.84383998402\n",
            "Epoch 14/50 loss =  7.247049875006269 gradiant norm =  76937.3096728084\n",
            "Epoch 15/50 loss =  7.230436112882549 gradiant norm =  76461.93403707689\n",
            "Epoch 16/50 loss =  7.216144352563379 gradiant norm =  76104.06088527112\n",
            "Epoch 17/50 loss =  7.205366406518054 gradiant norm =  75849.51781853811\n",
            "Epoch 18/50 loss =  7.1957607368316445 gradiant norm =  75619.31721003955\n",
            "Epoch 19/50 loss =  7.187826426320431 gradiant norm =  75426.85542143289\n",
            "Epoch 20/50 loss =  7.180986985921069 gradiant norm =  75257.8041109934\n",
            "Epoch 21/50 loss =  7.084594670091949 gradiant norm =  65853.50252235192\n",
            "Epoch 22/50 loss =  7.070214769588671 gradiant norm =  63124.276595421135\n",
            "Epoch 23/50 loss =  7.067480300977594 gradiant norm =  62902.43042051243\n",
            "Epoch 24/50 loss =  7.065656796409169 gradiant norm =  62774.93060651035\n",
            "Epoch 25/50 loss =  7.0645014622915605 gradiant norm =  62701.72686178886\n",
            "Epoch 26/50 loss =  7.063666519301157 gradiant norm =  62658.84240894542\n",
            "Epoch 27/50 loss =  7.063003292373278 gradiant norm =  62634.673217865595\n",
            "Epoch 28/50 loss =  7.062438202814623 gradiant norm =  62622.38992461132\n",
            "Epoch 29/50 loss =  7.061930032710612 gradiant norm =  62617.54485204963\n",
            "Epoch 30/50 loss =  7.061454542500849 gradiant norm =  62617.16762339301\n",
            "Epoch 31/50 loss =  7.05329411975132 gradiant norm =  61475.104310831586\n",
            "Epoch 32/50 loss =  7.0505179007795435 gradiant norm =  61437.21056390586\n",
            "Epoch 33/50 loss =  7.0499425734462715 gradiant norm =  61510.753283108694\n",
            "Epoch 34/50 loss =  7.049811956040094 gradiant norm =  61562.76634910589\n",
            "Epoch 35/50 loss =  7.0497864730135404 gradiant norm =  61594.4197995551\n",
            "Epoch 36/50 loss =  7.049790304729784 gradiant norm =  61615.47625339104\n",
            "Epoch 37/50 loss =  7.049802844906803 gradiant norm =  61631.487069786665\n",
            "Epoch 38/50 loss =  7.049817368104942 gradiant norm =  61644.99468023376\n",
            "Epoch 39/50 loss =  7.049831392622952 gradiant norm =  61657.09530573317\n",
            "Epoch 40/50 loss =  7.049843944621109 gradiant norm =  61668.257958637994\n",
            "Epoch 41/50 loss =  7.048338956242333 gradiant norm =  61376.17194019255\n",
            "Epoch 42/50 loss =  7.048314998928491 gradiant norm =  61388.3080277026\n",
            "Epoch 43/50 loss =  7.048296017237689 gradiant norm =  61399.93265276659\n",
            "Epoch 44/50 loss =  7.048281131190365 gradiant norm =  61411.0187123845\n",
            "Epoch 45/50 loss =  7.04826960110145 gradiant norm =  61421.55192648435\n",
            "Epoch 46/50 loss =  7.048260814511305 gradiant norm =  61431.529130357754\n",
            "Epoch 47/50 loss =  7.048254264757276 gradiant norm =  61440.955691273324\n",
            "Epoch 48/50 loss =  7.048249533112545 gradiant norm =  61449.8434148409\n",
            "Epoch 49/50 loss =  7.048246273903055 gradiant norm =  61458.20885517989\n",
            "Epoch 50/50 loss =  7.048244202110217 gradiant norm =  61466.07195745644\n"
          ]
        }
      ],
      "source": [
        "# Train your regression model\n",
        "w, bias = minibatch_2(train_x, train_y, train_config)\n",
        "# w = theta[:-1].reshape(-1,1)\n",
        "# bias = np.expand_dims(theta[-1], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019GwPMrbmrB"
      },
      "source": [
        "# **Testing:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1930,
      "metadata": {
        "id": "5FjQNzOb6BeQ"
      },
      "outputs": [],
      "source": [
        "def parse2test(data, feats):\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(90):\n",
        "    x_tmp = data[feats,8*i: 8*i+8]\n",
        "    x.append(x_tmp.reshape(-1,))\n",
        "    if i == 89:\n",
        "      # The last one is just a placeholder\n",
        "      y_tmp = 0\n",
        "    else:\n",
        "      y_tmp = data[-1, 8*i+8] # last column of (i+8)th row: PM2.5\n",
        "    y.append(y_tmp)\n",
        "\n",
        "\n",
        "  # x.shape: (n, 15, 8)\n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1931,
      "metadata": {
        "id": "hs6zgW-_IQFc"
      },
      "outputs": [],
      "source": [
        "def normalize_test_data(df, norm_params):\n",
        "    data_norm = df.copy()\n",
        "\n",
        "    for col, params in norm_params.items():\n",
        "        if col in df.columns:\n",
        "            data_norm[col] = (df[col] - params['mean']) / params['std']\n",
        "\n",
        "    return data_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1932,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "m00CNh3QHJP5",
        "outputId": "9455fe69-86b5-48b5-f2fb-445421acf0af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AMB_TEMP</th>\n",
              "      <th>CO</th>\n",
              "      <th>NO</th>\n",
              "      <th>NO2</th>\n",
              "      <th>NOx</th>\n",
              "      <th>O3</th>\n",
              "      <th>PM10</th>\n",
              "      <th>WS_HR</th>\n",
              "      <th>RAINFALL</th>\n",
              "      <th>RH</th>\n",
              "      <th>SO2</th>\n",
              "      <th>WD_HR</th>\n",
              "      <th>WIND_DIREC</th>\n",
              "      <th>WIND_SPEED</th>\n",
              "      <th>PM2.5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27.5</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.7</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.8</td>\n",
              "      <td>13.2</td>\n",
              "      <td>31.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>1.7</td>\n",
              "      <td>180.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27.2</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.4</td>\n",
              "      <td>15.7</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>192.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26.8</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.4</td>\n",
              "      <td>4.3</td>\n",
              "      <td>4.8</td>\n",
              "      <td>12.8</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>181.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26.7</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.4</td>\n",
              "      <td>4.1</td>\n",
              "      <td>4.5</td>\n",
              "      <td>12.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>179.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26.4</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.4</td>\n",
              "      <td>4.1</td>\n",
              "      <td>4.6</td>\n",
              "      <td>10.1</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>184.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>715</th>\n",
              "      <td>16.0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.9</td>\n",
              "      <td>4.2</td>\n",
              "      <td>47.1</td>\n",
              "      <td>34.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>130.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>716</th>\n",
              "      <td>15.6</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.4</td>\n",
              "      <td>3.3</td>\n",
              "      <td>3.7</td>\n",
              "      <td>44.1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>136.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>717</th>\n",
              "      <td>15.7</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>4.1</td>\n",
              "      <td>44.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>133.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>15.1</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.6</td>\n",
              "      <td>10.5</td>\n",
              "      <td>11.1</td>\n",
              "      <td>29.9</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>24.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>719</th>\n",
              "      <td>15.8</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.7</td>\n",
              "      <td>40.5</td>\n",
              "      <td>28.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>133.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>720 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     AMB_TEMP    CO   NO   NO2   NOx    O3  PM10  WS_HR  RAINFALL    RH  SO2  \\\n",
              "0        27.5  0.22  0.7   9.0   9.8  13.2  31.0    1.2       0.0  79.0  1.7   \n",
              "1        27.2  0.17  0.4   5.0   5.4  15.7  20.0    1.5       0.0  79.0  1.6   \n",
              "2        26.8  0.17  0.4   4.3   4.8  12.8  16.0    1.6       0.0  81.0  1.3   \n",
              "3        26.7  0.19  0.4   4.1   4.5  12.0  21.0    1.7       0.0  80.0  1.5   \n",
              "4        26.4  0.22  0.4   4.1   4.6  10.1  23.0    2.2       0.0  81.0  1.5   \n",
              "..        ...   ...  ...   ...   ...   ...   ...    ...       ...   ...  ...   \n",
              "715      16.0  0.26  0.3   3.9   4.2  47.1  34.0    2.7       0.0  70.0  0.5   \n",
              "716      15.6  0.25  0.4   3.3   3.7  44.1  27.0    3.0       0.0  74.0  0.6   \n",
              "717      15.7  0.24  0.4   3.7   4.1  44.1  29.0    2.9       0.0  73.0  0.6   \n",
              "718      15.1  0.24  0.6  10.5  11.1  29.9   9.0    0.8       0.0  95.0  0.6   \n",
              "719      15.8  0.28  0.6   6.0   6.7  40.5  28.0    3.0       0.0  74.0  0.5   \n",
              "\n",
              "     WD_HR  WIND_DIREC  WIND_SPEED  PM2.5  \n",
              "0    180.0       171.0         1.2   20.0  \n",
              "1    192.0       187.0         1.9    8.0  \n",
              "2    181.0       180.0         1.8    9.0  \n",
              "3    179.0       188.0         2.3    6.0  \n",
              "4    184.0       186.0         1.9    5.0  \n",
              "..     ...         ...         ...    ...  \n",
              "715  130.0       133.0         2.8   13.0  \n",
              "716  136.0       131.0         3.2   15.0  \n",
              "717  133.0       129.0         2.7   12.0  \n",
              "718   24.0        21.0         1.1    8.0  \n",
              "719  133.0       131.0         2.9   14.0  \n",
              "\n",
              "[720 rows x 15 columns]"
            ]
          },
          "execution_count": 1932,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = pd.read_csv('./content/test.csv')\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1933,
      "metadata": {
        "id": "NWff4h-MHQwT"
      },
      "outputs": [],
      "source": [
        "# Testing data preprocessing\n",
        "def test_processing(test_df, norm=False, norm_params=norm_params):\n",
        "    if norm:\n",
        "        if norm_params is None:\n",
        "            raise ValueError(\"norm_params required when norm=True\")\n",
        "        # Apply training normalization parameters to testing data\n",
        "        data_norm = normalize_test_data(test_df, norm_params)\n",
        "        data_values = data_norm.values\n",
        "    else:\n",
        "        # Use raw testing data\n",
        "        data_values = test_df.values\n",
        "\n",
        "    # Common processing steps\n",
        "    test_data = np.transpose(np.array(np.float64(data_values)))\n",
        "    test_x, test_y = parse2test(test_data, feats)\n",
        "\n",
        "    return test_x, test_y \n",
        "\n",
        "\n",
        "\n",
        "test_x, test_y = test_processing(test_df, norm=True, norm_params=norm_params)\n",
        "# print(np.max(test_x), np.min(test_x))\n",
        "# X = np.hstack((test_x, np.full(test_x.shape[0],1).reshape(-1,1)))\n",
        "# X_pinv = np.linalg.pinv(X)\n",
        "# theta = X_pinv @ test_y\n",
        "# loss = np.mean((test_y - X @ theta) ** 2)\n",
        "# print(f\"Loss (2nd Polynomial Regression): {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWrfEwaEdO6J"
      },
      "source": [
        "# **Write result as .csv**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1934,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqEQ1fZ9-WMO",
        "outputId": "05376f79-917f-4b0e-8b0a-f1cf1a8180d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss =  34.55748577080648\n"
          ]
        }
      ],
      "source": [
        "with open('my_sol.csv', 'w', newline='') as csvf:\n",
        "  writer = csv.writer(csvf)\n",
        "  writer.writerow(['Id','Predicted'])\n",
        "\n",
        "  loss_sum = 0.0\n",
        "  for i in range(int(test_x.shape[0])):\n",
        "    # Prediction of linear regression\n",
        "    if test_x.shape[1] != w.shape[0]:\n",
        "        x2 = test_x[i] ** 2\n",
        "        test_x_i = np.concatenate((test_x[i], x2), axis=0)\n",
        "    else:\n",
        "        test_x_i = test_x[i]\n",
        "    prediction = (np.dot(np.reshape(w,-1),test_x_i) + bias)[0]\n",
        "    loss_sum += (test_y[i] - prediction) ** 2\n",
        "    writer.writerow([i, prediction])\n",
        "  print(\"Test loss = \", loss_sum/test_x.shape[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
