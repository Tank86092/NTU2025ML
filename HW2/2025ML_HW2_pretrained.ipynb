{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDVwVcC2ofHK"
      },
      "source": [
        "# ML HW2 sample code\n",
        "TODO:\n",
        " - Design your CNN model (you are welcomed to use existing models)\n",
        " - Hyperparameter tuning\n",
        " - Confusion matrix\n",
        "\n",
        "Report:\n",
        " - Structure of CNN model\n",
        " - Data Augmentation\n",
        " - Draw the confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MFWlImM-3FM"
      },
      "source": [
        "#### Download data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfyxtZsqrw-_",
        "outputId": "4de3ae7c-d138-46b6-ed8d-c14a8a2deacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=19oEShkdcBJf41nu46anyvu7BgXUffIGi\n",
            "From (redirected): https://drive.google.com/uc?id=19oEShkdcBJf41nu46anyvu7BgXUffIGi&confirm=t&uuid=38dc1d87-f656-41b5-af68-05a25231f3a0\n",
            "To: /content/HW2.zip\n",
            "100% 52.1M/52.1M [00:00<00:00, 135MB/s]\n",
            "replace data/test/5659.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!gdown 19oEShkdcBJf41nu46anyvu7BgXUffIGi\n",
        "!unzip -q 'HW2.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rV0FCmEOSz0"
      },
      "source": [
        "#### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "sMsNXtVLopQ2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torchsummary import summary\n",
        "from torchvision import models\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from argparse import Namespace\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbV9AEejOSz2"
      },
      "source": [
        "#### Hyperparameters and setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYrXapu1OOER"
      },
      "outputs": [],
      "source": [
        "# TODO: modify the hyperparameters\n",
        "config = Namespace(\n",
        "    random_seed = 102,\n",
        "    BATCH = 128,\n",
        "    n_epoch = 15,\n",
        "    lr = 1e-3,\n",
        "    weight_decay = 1e-7,\n",
        "    ckpt_path = 'model.pth',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9hIGGaiOSz2"
      },
      "outputs": [],
      "source": [
        "TRA_PATH = 'data/train/'\n",
        "TST_PATH = 'data/test/'\n",
        "LABEL_PATH = 'data/train.csv'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(config.random_seed)\n",
        "torch.cuda.manual_seed_all(config.random_seed)\n",
        "random.seed(config.random_seed)\n",
        "np.random.seed(config.random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voXR5jPVPnxp"
      },
      "source": [
        "#### Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyMfGXwgMWQV"
      },
      "outputs": [],
      "source": [
        "class FaceExpressionDataset(Dataset):\n",
        "    def __init__(self, img_path, label_path=None, tfm=T.ToTensor()):\n",
        "        n_samples = len(os.listdir(img_path))\n",
        "        if label_path is not None:\n",
        "            self.images = [f'{img_path}/{i+7000}.jpg' for i in range(n_samples)]\n",
        "            self.labels = pd.read_csv(label_path)['label'].values.tolist()\n",
        "        else:\n",
        "            self.images = [f'{img_path}/{i}.jpg' for i in range(n_samples)]\n",
        "            self.labels = None\n",
        "        self.tfm = tfm\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx])\n",
        "        img = self.tfm(img)\n",
        "        if self.labels is not None:\n",
        "            lab = torch.tensor(self.labels[idx]).long()\n",
        "            return img, lab\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbaUJhVn3EwX"
      },
      "outputs": [],
      "source": [
        "# TODO: define your augmentation for training and evaluation\n",
        "train_tfm = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomAffine(degrees=30, scale=(0.90, 1.1)),\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.05),\n",
        "    T.ToTensor(),\n",
        "\n",
        "])\n",
        "eval_tfm = T.Compose([\n",
        "    T.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPeXOxjm3AkA"
      },
      "outputs": [],
      "source": [
        "# dataset\n",
        "train_dataset = FaceExpressionDataset(TRA_PATH, LABEL_PATH, tfm=train_tfm)\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [0.8, 0.2])\n",
        "test_dataset = FaceExpressionDataset(TST_PATH, tfm=eval_tfm)\n",
        "# dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.BATCH, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=config.BATCH, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.BATCH, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "\n",
        "def show_batch(images, labels, classes=None, nrow=8, title=\"Sample Training Batch\"):\n",
        "    \"\"\"\n",
        "    Visualize a batch of images from a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        images (Tensor): Batch of images [B, C, H, W]\n",
        "        labels (Tensor): Corresponding labels [B]\n",
        "        classes (list, optional): Class names corresponding to label indices.\n",
        "        nrow (int): Number of images per row in the grid.\n",
        "        title (str): Title for the plot.\n",
        "    \"\"\"\n",
        "    # Denormalize (if you used normalization in transforms)\n",
        "    img = images.clone()\n",
        "    select = [i for i, x in enumerate(labels.tolist()) if x == 2]\n",
        "    img = img[select]\n",
        "\n",
        "\n",
        "    # Make grid\n",
        "    grid = torchvision.utils.make_grid(img, nrow=nrow, padding=2)\n",
        "    plt.figure(figsize=(nrow * 0.72, nrow * 0.72))\n",
        "    plt.imshow(grid.permute(1, 2, 0))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Add labels below each image\n",
        "    if classes is not None:\n",
        "        label_texts = [classes[l.item()] if isinstance(l.item(), int) else str(l) for l in labels[:nrow]]\n",
        "        plt.title(f\"{title}\\n\" + \" | \".join(label_texts), fontsize=10)\n",
        "    else:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Get a batch\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# Define your class names (adjust to your dataset)\n",
        "classes = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
        "\n",
        "# Visualize\n",
        "show_batch(images, labels, classes=classes)"
      ],
      "metadata": {
        "id": "ypdXGbl4-CGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLA_KBidRF73"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QwewjIWP39y"
      },
      "outputs": [],
      "source": [
        "# TODO: define your CNN model\n",
        "def FaceExpressionNet():\n",
        "    x = models.resnet18()\n",
        "    x.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "    x.fc = nn.Linear(x.fc.in_features, 7)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyyjQS2eOSz7"
      },
      "source": [
        "#### training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8srGQLzloEv"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, valid_loader, config):\n",
        "    model.to(device)\n",
        "    criteria = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.3, patience=1\n",
        "    )\n",
        "\n",
        "    best_acc = 0\n",
        "    train_losses, valid_losses = [], []\n",
        "    for epoch in range(config.n_epoch):\n",
        "        # train\n",
        "        model.train()\n",
        "        train_loss, train_acc = 0, 0\n",
        "        for img, lab in tqdm(train_loader):\n",
        "            img, lab = img.to(device), lab.to(device)\n",
        "            output = model(img)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criteria(output, lab)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            train_acc += (torch.argmax(output, dim=-1) == lab).float().mean().item()\n",
        "        train_loss, train_acc = train_loss/len(train_loader), train_acc/len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        print(f'Epoch: {epoch+1}/{config.n_epoch}, train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')\n",
        "\n",
        "        # valid\n",
        "        model.eval()\n",
        "        valid_loss, valid_acc = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for img, lab in valid_loader:\n",
        "                img, lab = img.to(device), lab.to(device)\n",
        "                output = model(img)\n",
        "                loss = criteria(output, lab)\n",
        "                valid_loss += loss.item()\n",
        "                valid_acc += (torch.argmax(output, dim=-1) == lab).float().mean().item()\n",
        "        valid_loss, valid_acc = valid_loss/len(valid_loader), valid_acc/len(valid_loader)\n",
        "        valid_losses.append(valid_loss)\n",
        "        scheduler.step(valid_loss)\n",
        "        print(optimizer.param_groups[0]['lr'])\n",
        "        print(f'Epoch: {epoch+1}/{config.n_epoch}, valid loss: {valid_loss:.4f}, valid acc: {valid_acc:.4f}')\n",
        "\n",
        "        # update\n",
        "        if valid_acc > best_acc:\n",
        "            best_acc = valid_acc\n",
        "            torch.save(model.state_dict(), config.ckpt_path)\n",
        "            print(f'== best valid acc: {best_acc:.4f} ==')\n",
        "    model.load_state_dict(torch.load(config.ckpt_path))\n",
        "\n",
        "    # plot the training/validation loss curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(config.n_epoch), train_losses, label='Training Loss')\n",
        "    plt.plot(range(config.n_epoch), valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j8aa5BG2hNa"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8ldWxIeG2SI",
        "outputId": "65e070e2-48c2-410b-da80-f44298290c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           3,136\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
            "            Conv2d-5           [-1, 64, 16, 16]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
            "              ReLU-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8           [-1, 64, 16, 16]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-11           [-1, 64, 16, 16]               0\n",
            "           Conv2d-12           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 16, 16]             128\n",
            "             ReLU-14           [-1, 64, 16, 16]               0\n",
            "           Conv2d-15           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 16, 16]             128\n",
            "             ReLU-17           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-18           [-1, 64, 16, 16]               0\n",
            "           Conv2d-19            [-1, 128, 8, 8]          73,728\n",
            "      BatchNorm2d-20            [-1, 128, 8, 8]             256\n",
            "             ReLU-21            [-1, 128, 8, 8]               0\n",
            "           Conv2d-22            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-23            [-1, 128, 8, 8]             256\n",
            "           Conv2d-24            [-1, 128, 8, 8]           8,192\n",
            "      BatchNorm2d-25            [-1, 128, 8, 8]             256\n",
            "             ReLU-26            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-27            [-1, 128, 8, 8]               0\n",
            "           Conv2d-28            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-29            [-1, 128, 8, 8]             256\n",
            "             ReLU-30            [-1, 128, 8, 8]               0\n",
            "           Conv2d-31            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-32            [-1, 128, 8, 8]             256\n",
            "             ReLU-33            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-34            [-1, 128, 8, 8]               0\n",
            "           Conv2d-35            [-1, 256, 4, 4]         294,912\n",
            "      BatchNorm2d-36            [-1, 256, 4, 4]             512\n",
            "             ReLU-37            [-1, 256, 4, 4]               0\n",
            "           Conv2d-38            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-39            [-1, 256, 4, 4]             512\n",
            "           Conv2d-40            [-1, 256, 4, 4]          32,768\n",
            "      BatchNorm2d-41            [-1, 256, 4, 4]             512\n",
            "             ReLU-42            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-43            [-1, 256, 4, 4]               0\n",
            "           Conv2d-44            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-45            [-1, 256, 4, 4]             512\n",
            "             ReLU-46            [-1, 256, 4, 4]               0\n",
            "           Conv2d-47            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-48            [-1, 256, 4, 4]             512\n",
            "             ReLU-49            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-50            [-1, 256, 4, 4]               0\n",
            "           Conv2d-51            [-1, 512, 2, 2]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-53            [-1, 512, 2, 2]               0\n",
            "           Conv2d-54            [-1, 512, 2, 2]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 2, 2]           1,024\n",
            "           Conv2d-56            [-1, 512, 2, 2]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-58            [-1, 512, 2, 2]               0\n",
            "       BasicBlock-59            [-1, 512, 2, 2]               0\n",
            "           Conv2d-60            [-1, 512, 2, 2]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-62            [-1, 512, 2, 2]               0\n",
            "           Conv2d-63            [-1, 512, 2, 2]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-65            [-1, 512, 2, 2]               0\n",
            "       BasicBlock-66            [-1, 512, 2, 2]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                    [-1, 7]           3,591\n",
            "================================================================\n",
            "Total params: 11,173,831\n",
            "Trainable params: 11,173,831\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 5.13\n",
            "Params size (MB): 42.62\n",
            "Estimated Total Size (MB): 47.77\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91/91 [00:24<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15, train loss: 1.6462, train acc: 0.3498\n",
            "0.001\n",
            "Epoch: 1/15, valid loss: 1.5296, valid acc: 0.4019\n",
            "== best valid acc: 0.4019 ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91/91 [00:24<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/15, train loss: 1.4277, train acc: 0.4476\n",
            "0.001\n",
            "Epoch: 2/15, valid loss: 1.5187, valid acc: 0.4264\n",
            "== best valid acc: 0.4264 ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91/91 [00:24<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3/15, train loss: 1.3297, train acc: 0.4918\n",
            "0.001\n",
            "Epoch: 3/15, valid loss: 1.3658, valid acc: 0.4838\n",
            "== best valid acc: 0.4838 ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91/91 [00:24<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4/15, train loss: 1.2667, train acc: 0.5135\n",
            "0.001\n",
            "Epoch: 4/15, valid loss: 1.3434, valid acc: 0.4865\n",
            "== best valid acc: 0.4865 ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91/91 [00:24<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5/15, train loss: 1.2115, train acc: 0.5372\n",
            "0.001\n",
            "Epoch: 5/15, valid loss: 1.3361, valid acc: 0.4807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91/91 [00:25<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6/15, train loss: 1.1797, train acc: 0.5486\n",
            "0.001\n",
            "Epoch: 6/15, valid loss: 1.2664, valid acc: 0.5194\n",
            "== best valid acc: 0.5194 ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91/91 [00:24<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7/15, train loss: 1.1418, train acc: 0.5661\n"
          ]
        }
      ],
      "source": [
        "model = FaceExpressionNet().to(device)\n",
        "\n",
        "summary(model, (1, 64, 64))\n",
        "train(model, train_loader, valid_loader, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fr6guYSLT1w"
      },
      "outputs": [],
      "source": [
        "def draw_confusion_matrix(model, valid_loader):\n",
        "    predictions, labels = [], []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for img, lab in tqdm(valid_loader):\n",
        "            img = img.to(device)\n",
        "            output = model(img)\n",
        "            predictions += torch.argmax(output, dim=-1).tolist()\n",
        "            labels += lab.tolist()\n",
        "    # TODO draw the confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import seaborn as sns\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "draw_confusion_matrix(model, valid_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrrI2-5M2PXe"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UXmJOoeM9UJ"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    predictions = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for img in tqdm(test_loader):\n",
        "            img = img.to(device)\n",
        "            output = model(img)\n",
        "            predictions += torch.argmax(output, dim=-1).tolist()\n",
        "    with open('predict.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['id', 'label'])\n",
        "        for id, r in enumerate(predictions):\n",
        "            writer.writerow([id, r])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqtDLxvkOSz9"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('model.pth'))\n",
        "test(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}